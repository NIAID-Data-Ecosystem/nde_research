{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d9db57",
   "metadata": {},
   "source": [
    "## Evaluate the results of ChatGPT\n",
    "\n",
    "Currently, The J-sim, R, and P are low between the ground truth and GPT predictions due to GPT being more precise or more broad in its selection of terms\n",
    "\n",
    "We need a systematic evaluation metrics to compare the results without having to constantly manually run them\n",
    "\n",
    "The approach:\n",
    "1. Calculate J-similarity, precision, recall and then remove the terms in common\n",
    "2. Check if a prediction term is within the same tree as a ground truth term\n",
    "3. If two terms are within the same tree:\n",
    "  * Treat the two terms as a match\n",
    "  * Determine which term is closer to the root\n",
    "    * Calculate a weight to apply depending on how close the closest term is to the root (Weight 1)\n",
    "      * If closest term is one step away from root it should have a lower score than a closest term that is 2 steps away. This is to lower the weight of excessively generic terms (use 1-(1/(# of steps to closest term))\n",
    "    * Calculate a weight to apply depending on the number of steps between the two 'matching' terms (Weight 2)\n",
    "      * If the closest term to root is the ground truth term, use (1/(# of steps between the terms))\n",
    "      * If the closest term to root is the prediction term, use -(1/(# of steps between the terms)): It is negative only to ensure we will later be able to inspect the directionality\n",
    "  * The overall absolute weight should be a combination of the two, for example:\n",
    "    * Overall absolute weight = Weight 1 + ABS(Weight 2)\n",
    "  * Weighted similarity: Add the overall absolute weight for all matches, then add the j-sim (since we previously removed exact matches)\n",
    "\n",
    "Adjusting for prediction number biases:\n",
    "* The weighted similarity will be advantageous to ChatGPT 4 due to its tendency to dump every relevant term\n",
    "* To account for that, we can penalize it for making excessive guesses by multiplying the weighted similarity against the ratio of (# of gold standard terms)/(# of predicted terms).\n",
    "\n",
    "Evaluating whether the prediction has a tendency to be more specific or less specific than the gold standard\n",
    "* Broadness evaluation: (# of positive Weight 2 values)/(# of negative Weight 2 values)\n",
    "  * If broadness evaluation is >1, LLM model predictions are more specific than Ground truth/gold standard terms\n",
    "  * If broadness evaluation is <1, LLM model predictions are less specific than ground truth/gold standard terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b36f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "gpt_results_file = os.path.join(data_path,'gpt3_results.tsv')\n",
    "\n",
    "gpt_results = pd.read_csv(gpt_results_file,delimiter='\\t',header=0)\n",
    "print(gpt_results.head(n=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62337d9",
   "metadata": {},
   "source": [
    "## Generating data for Topic Category Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7401caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join('data','GPT categorization validation - data.tsv')\n",
    "data = pd.read_csv(datafile,delimiter = '\\t',header=0)\n",
    "print(data.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef15992",
   "metadata": {},
   "outputs": [],
   "source": [
    "namelist = data['Name'].unique().tolist()\n",
    "resultlist = []\n",
    "fail = []\n",
    "for eachname in namelist:\n",
    "    try:\n",
    "        r = requests.get(f'https://api.data.niaid.nih.gov/v1/query?q={eachname}&fields=_id,name')\n",
    "        result = json.loads(r.text)\n",
    "        hit = result['hits'][0]\n",
    "        url = f\"https://data.niaid.nih.gov/resources?id={hit['_id']}\"\n",
    "        resultlist.append({'_id': hit['_id'],'name':hit['name'],'url':url})\n",
    "    except:\n",
    "        fail.append(eachname)\n",
    "resultdf = pd.DataFrame(resultlist)\n",
    "resultdf.to_csv(os.path.join('result','topic_category_evaluation.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48505d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
